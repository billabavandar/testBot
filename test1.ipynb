{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# LangChain Core Mechanics: A Practical Study\n",
    "\n",
    "This document outlines a series of experiments conducted to gain practical experience with the LangChain library. The focus is on deconstructing and implementing key components essential for building LLM-driven applications.\n",
    "\n",
    "**The experimental process covers:**\n",
    "1.  Loading and segmenting textual data for LLM consumption.\n",
    "2.  Converting text segments into semantic vector representations (embeddings) via local Hugging Face models.\n",
    "3.  Indexing these embeddings within a FAISS vector store to enable rapid retrieval of relevant information.\n",
    "4.  Integrating the retrieval mechanism with a language model to perform context-aware question answering (Retrieval-Augmented Generation).\n",
    "\n",
    "The results and observations herein are part of an ongoing learning endeavor. While foundational, this exploration provides a basis for more advanced LangChain development.\n",
    "\n",
    "**Software Stack:** Python, LangChain, Hugging Face (`sentence-transformers`), FAISS."
   ],
   "id": "7e96d61d4eab0c0e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T07:27:01.598755Z",
     "start_time": "2025-06-07T07:27:01.474346Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "import tqdm\n",
    "warnings.filterwarnings('ignore')\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from typing import Any, List, Mapping, Optional\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain_core.language_models.llms import LLM\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "load_dotenv()"
   ],
   "id": "55995f2a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T07:27:03.072695Z",
     "start_time": "2025-06-07T07:27:02.983354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Claude2LLM(LLM):\n",
    "    n: int\n",
    "    model_to_use: str = \"anthropic/claude-3-haiku-20240307\" # Default model\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"deepseek/deepseek-r1-0528:free\"\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        OPENROUTER_API_KEY = os.getenv('OPENROUTER_API_KEY')\n",
    "        if not OPENROUTER_API_KEY:\n",
    "            raise ValueError(\"OPENROUTER_API_KEY not found in environment variables.\")\n",
    "\n",
    "        # Consider making YOUR_SITE_URL configurable or passed in __init__\n",
    "        YOUR_SITE_URL = 'https://github.com/billabavandar/testBot'\n",
    "        headers = {\n",
    "            'Authorization': f'Bearer {OPENROUTER_API_KEY}',\n",
    "            'HTTP-Referer': YOUR_SITE_URL,\n",
    "            'X-Title': 'LangChain Learning Project',\n",
    "            'Content-Type': 'application/json'\n",
    "        }\n",
    "        data = {\n",
    "            'model': self.model_to_use,\n",
    "            'messages': [{'role': 'user', 'content': prompt}]\n",
    "        }\n",
    "\n",
    "        response = requests.post('https://openrouter.ai/api/v1/chat/completions', headers=headers, data=json.dumps(data))\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            raise ValueError(f\"OpenRouter API request failed: {response.status_code} - {response.text}\")\n",
    "        try:\n",
    "            response_json = response.json()\n",
    "        except requests.exceptions.JSONDecodeError:\n",
    "            raise ValueError(f\"Failed to decode JSON from OpenRouter response: {response.text}\")\n",
    "\n",
    "        try:\n",
    "            output = response_json['choices'][0]['message']['content']\n",
    "        except (KeyError, IndexError, TypeError) as e:\n",
    "            print(f\"Error parsing OpenRouter response structure: {e}\")\n",
    "            print(f\"Full JSON response was: {response_json}\")\n",
    "            raise\n",
    "\n",
    "        if stop is not None:\n",
    "            # Currently not passing stop to API, this is a placeholder\n",
    "            print(\"Warning: 'stop' arguments were passed but are not implemented in this custom LLM.\")\n",
    "        return output\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        return {\"n\": self.n, \"model_name\": self.model_to_use}"
   ],
   "id": "a8c5da5e2a088941",
   "outputs": [],
   "execution_count": 71
  },
  {
   "cell_type": "code",
   "id": "712d9ce5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T07:27:05.798620Z",
     "start_time": "2025-06-07T07:27:04.768123Z"
    }
   },
   "source": [
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://www.promptingguide.ai/introduction/settings\",\n",
    "    \"https://python.langchain.com/v0.1/docs/modules/model_io/llms/\" # LangChain LLM docs\n",
    "]\n",
    "print(f\"Loading documents from {len(urls)} URLs...\")\n",
    "loader = WebBaseLoader(web_paths=urls)\n",
    "all_web_docs = []\n",
    "try:\n",
    "    all_web_docs = loader.load()\n",
    "    print(f\"Successfully loaded {len(all_web_docs)} documents from the web.\")\n",
    "    if not all_web_docs:\n",
    "        print(\"Warning: No documents were loaded. Check URLs and network connection.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading documents from web: {e}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents from 1 URLs...\n",
      "Successfully loaded 1 documents from the web.\n"
     ]
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T07:27:07.448069Z",
     "start_time": "2025-06-07T07:27:07.410403Z"
    }
   },
   "cell_type": "code",
   "source": [
    "split_docs = []\n",
    "if all_web_docs:\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=150\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(all_web_docs)\n",
    "    print(f\"Split web documents into {len(split_docs)} chunks.\")\n",
    "else:\n",
    "    print(\"Skipping document splitting as no documents were loaded.\")"
   ],
   "id": "bab4fd371637369e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split web documents into 1202 chunks.\n"
     ]
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T07:27:13.413624Z",
     "start_time": "2025-06-07T07:27:09.033986Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "model_name = \"BAAI/bge-large-en-v1.5\" # biggest baddest embedder\n",
    "\n",
    "embeddings_model = None\n",
    "if split_docs:\n",
    "    print(f\"Initializing HuggingFaceEmbeddings with model: {model_name}\")\n",
    "    embeddings_model = HuggingFaceEmbeddings(\n",
    "        model_name=model_name,\n",
    "        model_kwargs={'device': 'cpu'},\n",
    "        show_progress=True,\n",
    "        encode_kwargs={'normalize_embeddings': True}\n",
    "    )\n",
    "    print(\"HuggingFaceEmbeddings model initialized.\")\n",
    "else:\n",
    "    print(\"Skipping embedding model initialization as there are no split documents.\")"
   ],
   "id": "1c91cbfdc4bee20e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing HuggingFaceEmbeddings with model: BAAI/bge-large-en-v1.5\n",
      "HuggingFaceEmbeddings model initialized.\n"
     ]
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T07:49:42.968790Z",
     "start_time": "2025-06-07T07:29:06.002751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vector_store = None\n",
    "index_path = \"faiss_index_web_docs_v2\" # Use a distinct name\n",
    "\n",
    "if split_docs and embeddings_model:\n",
    "    print(f\"\\nCreating FAISS vector store, will save to: {index_path}\")\n",
    "    vector_store = FAISS.from_documents(split_docs, embeddings_model)\n",
    "    vector_store.save_local(index_path)\n",
    "    print(f\"FAISS vector store created and saved to {index_path}.\")\n",
    "else:\n",
    "    print(\"Skipping FAISS store creation/saving as prerequisites are not met.\")"
   ],
   "id": "2a98d23384c6197e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating FAISS vector store, will save to: faiss_index_harrypotter1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/38 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5ea9c4282b834c35ae3261e536a71c4c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS vector store created and saved to faiss_index_harrypotter1.\n"
     ]
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T07:50:15.752758Z",
     "start_time": "2025-06-07T07:50:15.747666Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ensure embeddings_model is defined from Cell 5 if you run this cell directly in a new session\n",
    "# This cell is for loading an already existing index.\n",
    "vector_store_loaded = None\n",
    "# index_path = \"faiss_index_web_docs_v2\" # Ensure this matches the save path\n",
    "\n",
    "# if not 'embeddings_model' in locals() or embeddings_model is None:\n",
    "#     print(\"Re-initializing embeddings model for loading FAISS index...\")\n",
    "#     model_name_for_load = \"BAAI/bge-small-en-v1.5\" # Must match the model used for saving\n",
    "#     embeddings_model = HuggingFaceEmbeddings(\n",
    "#         model_name=model_name_for_load,\n",
    "#         model_kwargs={'device': 'cpu'},\n",
    "#         show_progress=False, # No need for progress bar on just init\n",
    "#         encode_kwargs={'normalize_embeddings': True}\n",
    "#     )\n",
    "#     print(\"Embeddings model re-initialized.\")\n",
    "\n",
    "\n",
    "# if os.path.exists(index_path) and embeddings_model:\n",
    "#     print(f\"\\nLoading FAISS index from: {index_path}\")\n",
    "#     vector_store_loaded = FAISS.load_local(\n",
    "#         index_path,\n",
    "#         embeddings_model,\n",
    "#         allow_dangerous_deserialization=True\n",
    "#     )\n",
    "#     print(\"FAISS Index loaded from disk.\")\n",
    "# else:\n",
    "#     print(f\"FAISS index not found at {index_path} or embeddings model not ready. Please run Cell 6 first or ensure embeddings_model is initialized.\")\n",
    "\n",
    "# Determine which vector store to use: the newly created one or the loaded one\n",
    "# If Cell 6 was just run, vector_store will be populated.\n",
    "# If you are in a new session and intend to load, vector_store_loaded would be used.\n",
    "# For simplicity in a single pass notebook, we'll primarily use the one from Cell 6 if it ran.\n",
    "if 'vector_store' in locals() and vector_store is not None:\n",
    "    vector_store_to_use = vector_store\n",
    "    print(\"Using newly created/updated vector store for Q&A.\")\n",
    "# elif vector_store_loaded is not None:\n",
    "#     vector_store_to_use = vector_store_loaded\n",
    "#     print(\"Using loaded vector store from disk for Q&A.\")\n",
    "else:\n",
    "    vector_store_to_use = None\n",
    "    print(\"Error: No vector store available for Q&A. Run Cell 6 or ensure Cell 7 loads successfully.\")"
   ],
   "id": "425a4490c83aa37a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using newly created/updated vector store for Q&A.\n"
     ]
    }
   ],
   "execution_count": 77
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The above cell is to make re runs easier",
   "id": "fcff2021b5fe90dc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T07:50:53.780993Z",
     "start_time": "2025-06-07T07:50:19.243177Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if vector_store_to_use:\n",
    "    retriever = vector_store_to_use.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "    print(\"\\nInitializing LLM for Q&A...\")\n",
    "    llm = Claude2LLM(n=1, model_to_use=\"deepseek/deepseek-r1-0528:free\")\n",
    "\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Answering questions using RetrievalQA chain ---\")\n",
    "\n",
    "    questions_to_ask = [\n",
    "        \"What are the core components of an LLM-powered autonomous agent system as described by Lilian Weng?\",\n",
    "        \"Explain the role of the 'temperature' setting in language model prompting.\",\n",
    "        \"What is the base class for LLMs in LangChain?\"\n",
    "    ]\n",
    "\n",
    "    for question in questions_to_ask:\n",
    "        print(f\"\\nProcessing Question: {question}\")\n",
    "        try:\n",
    "            print(\"--- Retrieving relevant documents ---\")\n",
    "            retrieved_docs = retriever.get_relevant_documents(question)\n",
    "            # or in newer LangChain: retrieved_docs = retriever.invoke(question)\n",
    "            for i, doc in enumerate(retrieved_docs):\n",
    "                print(f\"  Retrieved Doc {i+1} (Source: {doc.metadata.get('source', 'N/A')}), Score (if available): {doc.metadata.get('score', 'N/A')}\")\n",
    "                print(f\"    Content snippet: {doc.page_content[:150]}...\")\n",
    "            print(\"--- End of retrieved documents ---\")\n",
    "\n",
    "            result = qa_chain.invoke({\"query\": question})\n",
    "            print(f\"LLM Answer: {result['result']}\")\n",
    "            print(\"\\nSource Documents actually used by LLM (might differ if chain does further processing):\")\n",
    "            for i, source_doc in enumerate(result['source_documents']): # These are the docs combined by 'stuff' chain\n",
    "                print(f\"  Source {i+1} (URL: {source_doc.metadata.get('source', 'N/A')}):\")\n",
    "                print(f\"    Content snippet: {source_doc.page_content[:250]}...\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing question '{question}': {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc() # Prints full traceback for the error\n",
    "        print(\"-\" * 40)\n",
    "else:\n",
    "    print(\"\\nSkipping Q&A as vector store is not available.\")\n",
    "\n",
    "print(\"\\n--- End of Notebook Execution ---\")"
   ],
   "id": "16d32f14fd324047",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing LLM for Q&A...\n",
      "\n",
      "--- Answering questions using RetrievalQA chain ---\n",
      "\n",
      "Processing Question: Who stole the Philosopher's stone\n",
      "--- Retrieving relevant documents ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a5bae39b5838441c814e42d3d47d6dd2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Retrieved Doc 1 (Source: https://hazidesaratcollege.ac.in/library/uploads/85jkr_harrypotter_1.pdf), Score (if available): N/A\n",
      "    Content snippet: �\u0001�\t����;q\u001A���\u0015�����Pi��b/�1���ϥ��p�9�gb\n",
      "��Rm\u000F�d\\�N\u0013�X=F\u0013�,8-D%�tN#��!h%9ed.��\u0016gB9�U��M�Hn�����ƌB��* ��t�\n",
      "�mpe'(\\(���&S6...Ԕ��A�\u000B���0\n",
      "  Retrieved Doc 2 (Source: https://hazidesaratcollege.ac.in/library/uploads/85jkr_harrypotter_1.pdf), Score (if available): N/A\n",
      "    Content snippet: startxref\r\n",
      "963612\r\n",
      "%%EOF\r\n",
      "1 0 obj<</CropBox[0 0 595 842]/Annots 1398 0 R/Parent 1264 0 R/Contents 3 0 R/Rotate 0/MediaBox[0 0 595 842]/Thumb 757 0 R/R...\n",
      "  Retrieved Doc 3 (Source: https://hazidesaratcollege.ac.in/library/uploads/85jkr_harrypotter_1.pdf), Score (if available): N/A\n",
      "endobj...j<</Parent 1309 0 R/A 1321 0 R/Next 1316 0 R/Prev 1320 0 R/Title(CHAPTER 13 - Nicolas Flamel)>>\n",
      "--- End of retrieved documents ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9337f7a1d20340c79007e14c9c8c4580"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Answer: Based solely on the provided context, it is impossible to determine who stole the Philosopher's Stone.\n",
      "\n",
      "Here's why:\n",
      "\n",
      "1.  **The Context is Fragmentary PDF Data:** The \"pieces of context\" you provided are garbled binary data and structural fragments extracted from a PDF file. This includes corrupted or unreadable streams, PDF object definitions (`obj`, `endobj`, `stream`, `endstream`), and metadata tags (like `Subject`, `Author`, `Creator`, `Title`).\n",
      "2.  **Title & Metadata Acknowledged, But Content Missing:** The metadata repeatedly identifies the document as \"*Harry Potter, Book 1; The Sorcerer's Stone*\", confirming the subject matter. There are also references to chapter titles like \"CHAPTER 13 - Nicolas Flamel\".\n",
      "3.  **Crucial Plot Elements Not Present:** While the metadata confirms the document is *about* Harry Potter and the Sorcerer's Stone, the specific narrative content revealing *who stole* the Stone is **not contained in the provided fragments**. The actual story text is absent from the snippets you shared. They are structural elements, some corrupted data, and metadata about the book, not the story itself.\n",
      "4.  **No Relevant Text Passages:** Scanning the fragments, there are no readable paragraphs or sentences from the book detailing the climax of the story or identifying the thief.\n",
      "\n",
      "**Conclusion:** The answer to \"Who stole the Philosopher's stone?\" is not found within the provided pieces of PDF metadata and structural data. I do not know the answer based on *this specific context*.\n",
      "\n",
      "*(Note: While I know the answer to the question in general from the Harry Potter story, your request specifically instructs me to use *only* the provided context or say I don't know. Based only on this context, I cannot find the answer).*\n",
      "\n",
      "Source Documents actually used by LLM (might differ if chain does further processing):\n",
      "  Source 1 (URL: https://hazidesaratcollege.ac.in/library/uploads/85jkr_harrypotter_1.pdf):\n",
      "    Content snippet: �\u0001�\t����;q\u001A���\u0015�����Pi��b/�1���ϥ��p�9�gb\n",
      "��Rm\u000F�d\\�N\u0013�X=F\u0013�,8-D%�tN#��!h%9ed.��\u0016gB9�U��M�Hn�����ƌB��* ��t�\n",
      "��$(q__�\u001A)Ս9�#�\\\u0019�~y���2�rO\u0017���9�Ʒ\u0000\u0003\u0000�\u0019q��n\u0002�NN\u0017-/*�&d5�4�7Bл�\u001B�䊑Lծ��\r\n",
      "e...\n",
      "  Source 2 (URL: https://hazidesaratcollege.ac.in/library/uploads/85jkr_harrypotter_1.pdf):\n",
      "    Content snippet: startxref\r\n",
      "963612\r\n",
      "%%EOF\r\n",
      "1306 0 obj<</PageMode/UseNone/ViewerPreferences<</DisplayDocTitle ...ts 3 0 R/Rotate 0/MediaBox[0 0 595 842]/Thumb 757 0 R/Resources 2 0 R/Type/Page>>\n",
      "  Source 3 (URL: https://hazidesaratcollege.ac.in/library/uploads/85jkr_harrypotter_1.pdf):\n",
      "1320 0 obj<</Parent 1309 0 R/A 1323 0 R/Next 1318 0 R/P... 1320 0 R/Title(CHAPTER 13 - Nicolas Flamel)>>\n",
      "----------------------------------------\n",
      "\n",
      "--- End of Notebook Execution ---\n"
     ]
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T07:52:25.023883Z",
     "start_time": "2025-06-07T07:52:07.818327Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Final Q&A Cell (Single Shot)\n",
    "\n",
    "my_question = \"What are the key components of an LLM agent system?\"\n",
    "if 'qa_chain' in locals() and qa_chain is not None:\n",
    "    if my_question and my_question.strip():\n",
    "        print(f\"\\nProcessing Question: \\\"{my_question}\\\"\")\n",
    "        try:\n",
    "            result = qa_chain.invoke({\"query\": my_question})\n",
    "\n",
    "            print(\"\\nLLM Answer:\")\n",
    "            answer_text = result.get('result', \"No answer found or 'result' key missing in response.\")\n",
    "            print(answer_text)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing your question: {e}\")\n",
    "\n",
    "            print(\"-\" * 40)\n",
    "    else:\n",
    "        print(\"The variable 'my_question' is empty. Please set a question.\")\n",
    "else:\n",
    "    print(\"Error: 'qa_chain' is not defined. Please run the setup cells that initialize 'qa_chain' first.\")"
   ],
   "id": "96472e0b46224e90",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Question: \"Who stole Neville's remebrall?\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b45353ee0e4940368db1d58e52549db9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LLM Answer:\n",
      "Based solely on the provided context, I cannot determine who stole Neville's Remembrall.\n",
      "\n",
      "**Here's why:**\n",
      "\n",
      "1.  **The context provided is not the text of the book:** The text you provided appears to be corrupted PDF metadata and structural data (like bookmarks and object references). It contains chapter titles (like \"CHAPTER 11 - Quidditch\") but **does not contain the actual narrative text** of \"Harry Potter and the Sorcerer's Stone\" where events like the theft of Neville's Remembrall are described.\n",
      "2.  **No mention of the event or characters:** The context does not contain the words \"Neville\", \"Remembrall\", names of characters involved (like Malfoy), or any description of the flying lesson where the incident occurred.\n",
      "3.  **Only chapter outlines:** The relevant section only lists chapter titles (e.g., Chapter 9 \"The Midnight Duel\", Chapter 10 \"Halloween\", Chapter 11 \"Quidditch\"). While the Remembrall scene happens *around* the flying lessons early in the book (relevant to the \"Quidditch\" chapter), the context doesn't provide any details *within* those chapters.\n",
      "\n",
      "**Therefore, the answer to your question is not contained in the pieces of context provided. I do not know the answer based on this information.**\n"
     ]
    }
   ],
   "execution_count": 80
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6b669c42cc39dc9d"
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
